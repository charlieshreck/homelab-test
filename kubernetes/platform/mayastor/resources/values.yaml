# Mayastor configuration for 3-node cluster with dual NICs
# Storage network: 10.11.0.0/24

# IO Engine configuration (data plane)
io_engine:
  # Huge pages (configured in Talos with vm.nr_hugepages=1024)
  hugepages:
    enabled: true
  # Resource limits per Mayastor best practices
  # Workers have 4 cores each, allowing 2 for IO engine + 2 for workloads
  resources:
    limits:
      cpu: "2"              # 2 dedicated CPU cores per node (Mayastor recommended)
      memory: "2Gi"
      hugepages-2Mi: "2Gi"  # 1024 x 2MiB huge pages
    requests:
      cpu: "1"              # Reduced request to allow node scheduling flexibility
      memory: "2Gi"
      hugepages-2Mi: "2Gi"
  # Target specific CPU cores for better performance (optional)
  # Uncomment and adjust based on your CPU topology
  # coreList: "2,3"  # Use cores 2 and 3

# Use all 3 worker nodes for Mayastor
agents:
  core:
    nodeSelector:
      openebs.io/engine: mayastor

# Storage pool configuration
# Each worker has a dedicated disk /dev/sdb for Mayastor (helford storage - 1TB total, ~300GB per node)
# DiskPools are automatically created via diskpools.yaml (deployed by ArgoCD)

# Resource limits appropriate for 9GB RAM per node
csi:
  node:
    resources:
      limits:
        cpu: "1"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory: "128Mi"

# Enable etcd for Mayastor control plane
# Use emptyDir with lifecycle hooks to clean up stale data on restarts
etcd:
  replicaCount: 3
  persistence:
    enabled: false

  # Use regular emptyDir (not Memory) to preserve data across pod restarts
  extraVolumes:
    - name: etcd-data
      emptyDir: {}

  extraVolumeMounts:
    - name: etcd-data
      mountPath: /bitnami/etcd/data

  # Init container to clean up stale data that causes bootstrap conflicts
  initContainers:
    - name: cleanup-stale-data
      image: busybox:latest
      command:
        - sh
        - -c
        - |
          # Remove member directory that contains stale bootstrap data
          # This allows etcd to rejoin the cluster cleanly after crashes
          if [ -d /bitnami/etcd/data/member ]; then
            echo "Removing stale member directory to prevent bootstrap conflicts"
            rm -rf /bitnami/etcd/data/member
          fi
          echo "Cleanup complete"
      volumeMounts:
        - name: etcd-data
          mountPath: /bitnami/etcd/data

  # Increased timeouts for stability (from 250/1250 to 500/2500)
  extraEnvVars:
    - name: ETCD_HEARTBEAT_INTERVAL
      value: "500"
    - name: ETCD_ELECTION_TIMEOUT
      value: "2500"

  # Spread etcd pods across different nodes for stability
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - etcd
            topologyKey: kubernetes.io/hostname

  resources:
    limits:
      cpu: "500m"
      memory: "512Mi"
    requests:
      cpu: "100m"
      memory: "256Mi"

  pdb:
    create: true
    minAvailable: 2

# Disable Loki (observability) - also needs storage, not required for core functionality
loki:
  enabled: false

# Enable metrics
metrics:
  enabled: true
